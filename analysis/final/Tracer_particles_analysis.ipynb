{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following the walkthrough on http://soft-matter.github.io/trackpy/v0.4.2/tutorial/walkthrough.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import detrend\n",
    "\n",
    "# change the following to %matplotlib notebook for interactive plotting\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "# Optionally, tweak styles.\n",
    "mpl.rc('figure',  figsize=(10, 5))\n",
    "mpl.rc('image', cmap='gray')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame, Series  # for convenience\n",
    "\n",
    "import cv2\n",
    "import imutils\n",
    "\n",
    "import pims\n",
    "import trackpy as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of frames\n",
    "N = 950\n",
    "\n",
    "# bitdepth\n",
    "bitdepth = 16\n",
    "\n",
    "# particle size (Large = 1, Medium/Small = 0)\n",
    "Size = 0\n",
    "\n",
    "# startframe number\n",
    "startframe = 1\n",
    "\n",
    "\n",
    "m = 7 #number of row in start calibration grid\n",
    "n = 9 #number of columns in start calibration grid\n",
    "\n",
    "\n",
    "# filename calibration file at the start\n",
    "path = 'C:/Users/s131431/Documents/Thesis/Opnames/2021-03-08/start_calibration.tiff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# If image has bitdepth 8 or it has sharp contrasts making grayscale values obsolete\n",
    "if bitdepth == 8:\n",
    "    image = cv2.imread(path,cv2.CV_8UC1) * 16\n",
    "# \n",
    "elif bitdepth == 16:\n",
    "    image = cv2.imread(path,cv2.CV_16UC1)\n",
    "\n",
    "\n",
    "# Fixed thresholding\n",
    "meanvalue = int(np.mean(image))\n",
    "thresholdvalue = meanvalue/1.5\n",
    "binaryImage = cv2.threshold(image,thresholdvalue,255,cv2.THRESH_BINARY_INV)[1]\n",
    "\n",
    "\n",
    "# Define a course kernel\n",
    "kernelClosed = np.ones((7,7),np.uint8)\n",
    "kernelOpen   = np.ones((5,5),np.uint8)\n",
    "\n",
    "# Reduce noise inside particles\n",
    "binaryImageClosed = cv2.morphologyEx(binaryImage, cv2.MORPH_CLOSE, kernelClosed)\n",
    "\n",
    "\n",
    "# Reduce noise outside particles\n",
    "binaryImageOpened = cv2.morphologyEx(binaryImageClosed, cv2.MORPH_OPEN, kernelOpen)\n",
    "markers = cv2.connectedComponents(np.uint8(binaryImageOpened))[1]\n",
    "Nmarkers = len(np.unique(markers))\n",
    "Ndots = int(n*m)\n",
    "maximumSize = 80\n",
    "minimumSize = 30\n",
    "xdots = np.empty(0,dtype=float)\n",
    "ydots = np.empty(0,dtype=float)\n",
    "\n",
    "for i in range(1,Nmarkers,1):\n",
    "    idx = np.argwhere(markers==i)\n",
    "    \n",
    "    xSpread = np.max(idx[:,1])-np.min(idx[:,1])    \n",
    "    ySpread = np.max(idx[:,0])-np.min(idx[:,0])\n",
    "    \n",
    "    if ((minimumSize < xSpread < maximumSize) and (minimumSize < ySpread < maximumSize)):\n",
    "        xdots = np.append(xdots,np.mean(idx[:,1]))\n",
    "        ydots = np.append(ydots,np.mean(idx[:,0]))\n",
    "    \n",
    "\n",
    "# Sort the dots\n",
    "NRows = m\n",
    "NColumns = n\n",
    "\n",
    "for i in range(0,NRows,1):\n",
    "    idx = np.argsort(xdots[i*NColumns:(i+1)*NColumns])\n",
    "    \n",
    "    xdots[i*NColumns:(i+1)*NColumns] = xdots[i*NColumns+idx]\n",
    "    ydots[i*NColumns:(i+1)*NColumns] = ydots[i*NColumns+idx]\n",
    "    \n",
    "\n",
    "Z2 = (ydots[n-1]-ydots[0])/(xdots[n-1]-xdots[0])\n",
    "\n",
    "phi = np.arctan(Z2)\n",
    "phi = phi*180/np.pi\n",
    "\n",
    "print(phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Read image\n",
    "image = cv2.imread(path)\n",
    "\n",
    "\n",
    "Rotation = phi\n",
    "\n",
    "# If image has bitdepth 8 or it has sharp contrasts making grayscale values obsolete\n",
    "if bitdepth == 8:\n",
    "    image = cv2.imread(path,cv2.CV_8UC1) * 16\n",
    "# \n",
    "elif bitdepth == 16:\n",
    "    image = cv2.imread(path,cv2.CV_16UC1)\n",
    "\n",
    "# Show image\n",
    "image = imutils.rotate(image, angle=Rotation)\n",
    "#plt.figure()\n",
    "#plt.imshow(image*1,origin='lower',cmap='gray')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "#plt.figure()\n",
    "#plt.imshow(image*1,origin='lower',cmap='gray')\n",
    "#plt.show()\n",
    "\n",
    "# Fixed thresholding\n",
    "meanvalue = int(np.mean(image))\n",
    "thresholdvalue = meanvalue/1.5\n",
    "binaryImage = cv2.threshold(image,thresholdvalue,255,cv2.THRESH_BINARY_INV)[1]\n",
    "\n",
    "# Define a course kernel\n",
    "kernelClosed = np.ones((7,7),np.uint8)\n",
    "kernelOpen   = np.ones((5,5),np.uint8)\n",
    "\n",
    "# Reduce noise inside particles\n",
    "binaryImageClosed = cv2.morphologyEx(binaryImage, cv2.MORPH_CLOSE, kernelClosed)\n",
    "\n",
    "# Reduce noise outside particles\n",
    "binaryImageOpened = cv2.morphologyEx(binaryImageClosed, cv2.MORPH_OPEN, kernelOpen)\n",
    "markers = cv2.connectedComponents(np.uint8(binaryImageOpened))[1]\n",
    "Nmarkers = len(np.unique(markers))\n",
    "Ndots = int(n*m)     #change for the number of dots on the calibration grid\n",
    "maximumSize = 80\n",
    "minimumSize = 30\n",
    "xdots = np.empty(0,dtype=float)\n",
    "ydots = np.empty(0,dtype=float)\n",
    "\n",
    "for i in range(1,Nmarkers,1):\n",
    "    idx = np.argwhere(markers==i)\n",
    "    \n",
    "    xSpread = np.max(idx[:,1])-np.min(idx[:,1])    \n",
    "    ySpread = np.max(idx[:,0])-np.min(idx[:,0])\n",
    "    \n",
    "    if ((minimumSize < xSpread < maximumSize) and (minimumSize < ySpread < maximumSize)):\n",
    "        xdots = np.append(xdots,np.mean(idx[:,1]))\n",
    "        ydots = np.append(ydots,np.mean(idx[:,0]))\n",
    "    \n",
    "plt.figure()\n",
    "plt.imshow(image,origin='lower',cmap='gray')\n",
    "plt.plot(xdots,ydots,'r.',markersize=1)\n",
    "plt.show()\n",
    "\n",
    "# Sort the dots\n",
    "NRows = m\n",
    "NColumns = n\n",
    "\n",
    "for i in range(0,NRows,1):\n",
    "    idx = np.argsort(xdots[i*NColumns:(i+1)*NColumns])\n",
    "    \n",
    "    xdots[i*NColumns:(i+1)*NColumns] = xdots[i*NColumns+idx]\n",
    "    ydots[i*NColumns:(i+1)*NColumns] = ydots[i*NColumns+idx]\n",
    "    \n",
    "# Positions of the dots in meters\n",
    "distanceDots = 0.02 # meters\n",
    "x = np.linspace(0,(NColumns-1)*distanceDots,NColumns)\n",
    "y = np.linspace(0,(NRows-1)   *distanceDots,NRows)\n",
    "xReal, yReal = np.meshgrid(x,y)\n",
    "\n",
    "# We need these two arrays as input data sets for the least square fitting\n",
    "Bx = xReal.flatten()\n",
    "By = yReal.flatten()\n",
    "\n",
    "# Positions of the dots in pixels\n",
    "X = xdots*1 - xdots[0]\n",
    "Y = ydots*1 - ydots[0]\n",
    "A = np.array([X*0+1, X, Y, X**2, X**2*Y, X**2*Y**2, Y**2, X*Y**2, X*Y]).T\n",
    "\n",
    "# Use linear least square fitting to find the coefficients that solve A*x=B\n",
    "coeffx, _, _, _ = np.linalg.lstsq(A,Bx,rcond=None)\n",
    "coeffy, _, _, _ = np.linalg.lstsq(A,By,rcond=None)\n",
    "\n",
    "# This is the function we just used for fitting, but here explicit\n",
    "def surface(x,y, a,b,c,d,e,f,g,h,i):\n",
    "    return a + b*x + c*y + d*x**2 + e*x**2*y + f*x**2*y**2 + g*y**2 + h*x*y**2 + i*x*y\n",
    "\n",
    "# This is the function to obtain the physical coordinates from the pixel coordinates\n",
    "def conversion(X,Y):\n",
    "    X1 = surface(X,Y,*coeffx)\n",
    "    Y1 = surface(X,Y,*coeffy)\n",
    "    return (X1,Y1)\n",
    "\n",
    "def xconversion(X,Y):\n",
    "    X1 = surface(X,Y,*coeffx)\n",
    "    Y1 = surface(X,Y,*coeffy)\n",
    "    return(X1)    \n",
    "def yconversion(X,Y):\n",
    "    X1 = surface(X,Y,*coeffx)\n",
    "    Y1 = surface(X,Y,*coeffy)\n",
    "    return(Y1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pims.pipeline\n",
    "def gray(image):\n",
    "    return image#[:, :, 1]  # Take just the green channel\n",
    "\n",
    "frames = pims.open('C:/Users/s131431/Documents/Thesis/Opnames/2021-03-08/curvelength700stroke45/curvelength700stroke45/z = 2/*.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames[123].frame_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames[123].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image conversion\n",
    "By default, `locate()` applies a bandpass and a percentile-based threshold to the image(s) before finding features. You can turn off this functionality using `preprocess=False, percentile=0.` In many cases, the default bandpass, which guesses good length scales from the `diameter` parameter, “just works.” But if you want to execute these steps manually, you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.find.percentile_threshold(frames[0],100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.preprocessing.invert_image(frames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.preprocessing.convert_to_int(frames[0])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locate features\n",
    "Start with just the first frame. Estimate the size of the features (in pixels). The size must be an odd integer, and it is better to err on the large side, as we'll see below. We estimate 11 pixels.\n",
    "\n",
    "The algorithm looks for bright features; since the features in this set of images are dark, we set `invert=True`.\n",
    "\n",
    "`locate` returns a spreadsheet-like object called a DataFrame. It lists\n",
    "- each feature's position,\n",
    "- various characterizations of its appearance, which we will use to filter out spurious features,\n",
    "- the \"signal\" strength and an estimate of uncertainty, both derived from this paper\n",
    "\n",
    "More information about DataFrames may be found in the pandas documentation. DataFrames can easily be exported to formats like CSV, Excel, SQL, HDF5, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 15\n",
    "f = tp.locate(tp.preprocessing.convert_to_int(frames[i])[1], 15, invert=True, topn=50)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.annotate(f,frames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.annotate(f,tp.invert_image(frames[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Refine parameters to elminate spurious features**\n",
    "\n",
    "Many of these circles are clearly wrong; they are fleeting peaks in brightness that aren't actually particles. Rejecting them often improves results and speeds up feature-finding. There are many ways to distinguish real particles from spurious ones. The most important way is to look at total brightness (\"mass\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(f['mass'], bins=20)\n",
    "\n",
    "# Optionally, label the axes.\n",
    "ax.set(xlabel='mass', ylabel='count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Check for subpixel accuracy**\n",
    "As Eric Weeks points out in his related tutorial, a quick way to check for subpixel accuracy is to check that the decimal part of the x and/or y positions are evenly distributed. Trackpy provides a convenience plotting function for this called `subpx_bias`. \n",
    "If we use a mask size that is too small, the histogram often shows a dip in the middle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp.subpx_bias(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **Locate features in all frames**\n",
    "Or, to start, just explore a subset of the frames.\n",
    "\n",
    "We'll locate features in the first 10 frames from this video. We use tp.batch, which calls tp.locate on each frame and collects the results.\n",
    "\n",
    "As each frame is analyzed, `tp.batch` reports the frame number and how many features were found. If this number runs unexpectedly low or high, you may wish to interrupt it and try different parameters.\n",
    "\n",
    "If your images are small, you may find that printing this number actually slows down batch significantly! In that case you can run `tp.quiet()` to turn it off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = tp.batch(frames[0:-1:1], 15, invert=True, topn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save('z2',np.asarray(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Step 3: Link features into particle trajectories**\n",
    "We have the locations of the particles in each frame. Next we'll track particles from frame to frame, giving each one a number for identification.\n",
    "\n",
    "First, we must must specify a maximum displacement, the farthest a particle can travel between frames. We should choose the smallest reasonable value because a large value slows computation time considerably. In this case, 5 pixels is reasonable.\n",
    "\n",
    "Second, we allow for the possibility that a particle might be missed for a few frames and then seen again. (Perhaps its \"mass\" slipped below our cutoff due to noise in the video.) Memory keeps track of disappeared particles and maintains their ID for up to some number of frames after their last appearance. Here we use 3 frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tp.link(f, 50, memory=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Filter spurious trajectories**\n",
    "We have more filtering to do. Ephemeral trajectories — seen only for a few frames — are usually spurious and never useful. The convenience function `filter_stubs` keeps only trajectories that last for a given number of frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = tp.filter_stubs(t, 50)\n",
    "# Compare the number of particles in the unfiltered and filtered data.\n",
    "print('Before:', t['particle'].nunique())\n",
    "print('After:', t1['particle'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also filter trajectories by their particles' appearance. At this stage, with trajectories linked, we can look at a feature's \"average appearance\" throughout its trajectory, giving a more accurate picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "tp.mass_size(t1.groupby('particle').mean()); # convenience function -- just plots size vs. mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "tp.plot_traj(t1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "tp.plot_displacements(t1,1,2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define fitting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x, a, b, c, d, e):\n",
    "\n",
    "    return a * np.cos( b * x + c) + d +e*x \n",
    "\n",
    "\n",
    "def func2(x, a, b, c):\n",
    "    return a*np.cos(b*x + c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the raw data in the x-direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "\n",
    "N = max(t1['particle'])\n",
    "\n",
    "for i in range(0,N,1):\n",
    "    t2 = t1[t1['particle'] == i]\n",
    "    \n",
    "    #maxframe = np.max(t2.frame)\n",
    "    #minframe = np.min(t2.frame)\n",
    "    \n",
    "    #if maxframe - minframe > 200:\n",
    "    \n",
    "    plt.plot(t2.frame,t2.x)\n",
    "#     plt.plot(t2.frame,t2.y)\n",
    "\n",
    "#     plt.plot(t2.frame,t2.x-np.mean(t2.x))\n",
    "#     plt.plot(t2.frame,t2.y-np.mean(t2.y))\n",
    "\n",
    "#xdum = np.linspace(0,430,1000)\n",
    "#plt.plot(xdum,-70*np.sin(xdum*0.145+np.pi),'k--')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and filter the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_frame_filter = 200\n",
    "max_cof_filter = 200\n",
    "error_ratio_filter = 0.99\n",
    "min_amplitude = 0 #start at 0 and then change it to the majority\n",
    "\n",
    "\n",
    "def rsquared(f,xdata,ydata,popt):\n",
    "    residuals = ydata - f(xdata,*popt)\n",
    "    ss_res = np.sum(residuals**2)\n",
    "    ss_tot = np.sum((ydata-np.mean(ydata))**2)\n",
    "    r_squared = 1 - (ss_res / ss_tot)\n",
    "    return r_squared\n",
    "\n",
    "\n",
    "plt.figure(dpi=150)\n",
    "\n",
    "N = max(t1['particle'])\n",
    "#maxtotalframe = 0\n",
    "#a_cof_total = 0\n",
    "#a_err_array = np.zeros(N)\n",
    "\n",
    "#a_array = np.zeros(N)\n",
    "#y_array = np.zeros(N)\n",
    "\n",
    "\n",
    "unique = np.unique(t1.particle.to_numpy())\n",
    "N = len(unique)\n",
    "a_cof = np.empty(0)\n",
    "a_error = np.empty(0)\n",
    "y_array = np.empty(0)\n",
    "\n",
    "for i in unique:\n",
    "    \n",
    "    \n",
    "    #i =22\n",
    "    \n",
    "    t2 = t1[t1['particle'] == i]\n",
    "    \n",
    "    \n",
    "    if max(t2.x)-min(t2.x) <= max_cof_filter:\n",
    "        continue\n",
    "    \n",
    "    #print(t2.frame)\n",
    "    #maxframe = np.max(t2.frame)\n",
    "    #minframe = np.min(t2.frame)\n",
    "    \n",
    "    #print(i)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #if maxframe - minframe > max_frame_filter:\n",
    "        #popt, _ = curve_fit(func, t2.frame, t2.x, )\n",
    "        #print(type(t2.x))\n",
    "        \n",
    "        #print(t2.x)\n",
    "    t3 = (t2.x).to_numpy()\n",
    "    #print(t2)\n",
    "    #print((t3))\n",
    "    \n",
    "\n",
    "    t_frame = (t2.frame).to_numpy()\n",
    "    #print(t4)\n",
    "\n",
    "\n",
    "\n",
    "    #x_new = detrend(t3)\n",
    "    #y_new = detrend(t4)\n",
    "\n",
    "    #print(max(x_new))\n",
    "    #print(t_frame)\n",
    "    #print(t2.x)\n",
    "    #popt, _ = curve_fit(func, t2.frame, t2.x, p0 = (70, 0.1,0, 360, 0.025 ) )#bounds=((0, 0, -20, 0),(80, 0.5, 20, 1200)))\n",
    "    popt, pcov = curve_fit(func, t_frame, t2.x, p0 = ((max(t2.x)-min(t2.x))/2, 0.15, 0, np.mean(t2.x),0))# bounds=((0, 0, -2*np.pi, -np.inf, -np.inf),(np.inf, np.inf, 2*np.pi, np.inf, np.inf)))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    b_cof = popt[1]\n",
    "    c_cof = popt[2]\n",
    "\n",
    "    perr = np.sqrt(np.diag(pcov))\n",
    "    \n",
    "  \n",
    "\n",
    "    \n",
    "    b_err = perr[1]\n",
    "    c_err = perr[2]\n",
    "\n",
    "    #print(a_cof)\n",
    "    r2 = rsquared(func, t2.frame, t2.x, popt)\n",
    "    #a_cof > max_cof_filter and\n",
    "    if   r2 > error_ratio_filter:\n",
    "        #print(r2)\n",
    "        \n",
    "        if abs(popt[0]) >= min_amplitude:\n",
    "            a_err = perr[0]\n",
    "            #print(perr[0])\n",
    "            \n",
    "            t4 = (t2.y).to_numpy()\n",
    "            \n",
    "            t4 = np.mean(t4)\n",
    "            #print(t4)\n",
    "            \n",
    "            y_array = np.append(y_array,t4 )\n",
    "            #y_array = np.append(y)\n",
    "\n",
    "                          \n",
    "            a_cof = np.append(a_cof,abs(popt[0]))\n",
    "            a_error = np.append(a_error, abs(perr[0]))\n",
    "\n",
    "            b_cof = popt[1]\n",
    "            c_cof = popt[2]\n",
    "            d_cof = popt[3]\n",
    "            e_cof = popt[4]\n",
    "            \n",
    "            #print(d_cof)\n",
    "            #print(e_cof)\n",
    "            \n",
    "            #print(a_cof)\n",
    "            print(popt)\n",
    "       \n",
    "            plt.plot(t2.frame, func(t2.frame, *popt), 'r-')\n",
    "        \n",
    "\n",
    "            #a_array[i] = a_cof\n",
    "            #y_array[i] = np.mean(t4)\n",
    "            #a_err_array[i] = a_err\n",
    "            #maxtotalframe = maxtotalframe + maxframe\n",
    "            #print(maxtotalframe)\n",
    "            #plt.plot(t_frame, x_new)\n",
    "            plt.plot(t_frame, t2.x)\n",
    "        #a_cof_total = a_cof_total + a_array[i]*maxframe\n",
    "          \n",
    "\n",
    "    #perr = np.sqrt(np.diag(pcov))\n",
    "\n",
    "#print(perr)\n",
    "\n",
    "#a_array = a_array[a_array != 0]\n",
    "#a_err_array = a_err_array[a_err_array != 0]\n",
    "\n",
    "\n",
    "#print(a_err_array)\n",
    "\n",
    "\n",
    "#std = (np.sum(((a_array - np.mean(a_array))**2)/(len(a_array)-1)))\n",
    "#print(a_cof_total/maxtotalframe)\n",
    "\n",
    "#Z = np.mean(a_array)\n",
    "#Y = np.std(a_array)\n",
    "\n",
    "#print(Z)\n",
    "#print(Y)\n",
    "\n",
    "#print(a_cof)\n",
    "#print(np.mean(a_cof))\n",
    "#print(np.std(a_cof))\n",
    "print(np.mean(a_cof))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtain the data in pixel coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean(a_cof))\n",
    "print(a_cof)\n",
    "\n",
    "print(y_array)\n",
    "\n",
    "error2 = np.sqrt(sum(a_error**2))\n",
    "\n",
    "error1 = np.std(a_cof)\n",
    "\n",
    "print(error1)\n",
    "print(error2)\n",
    " \n",
    "print(len(a_cof))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = xconversion(a_cof,y_array)\n",
    "X_mean = np.mean(X)\n",
    "\n",
    "\n",
    "#X_err = xconversion(a_err_array, y_array)\n",
    "#X_var = X_err**2\n",
    "#X_er = np.sqrt(np.mean(X_var))\n",
    "#print(X)\n",
    "#print(X_err)\n",
    "#print(X_var)\n",
    "print('Amplitude = ', format( X_mean))\n",
    "#\n",
    "#print('Error = ', format(X_er))\n",
    "\n",
    "\n",
    "fit_error = xconversion(a_error, y_array)\n",
    "\n",
    "\n",
    "print('Error_fitting = ', format(np.sqrt(sum((fit_error**2)))))\n",
    "\n",
    "print('Error = ', format(np.std(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
